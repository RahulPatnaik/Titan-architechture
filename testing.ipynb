{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 63232\n",
      "Output shape: torch.Size([2, 32, 64])\n",
      "Associative memory loss: 0.05827818438410759\n",
      "MAC output shape: torch.Size([2, 42, 64])\n",
      "MAL output shape: torch.Size([2, 32, 64])\n",
      "MAL associative memory loss: 0.04597906768321991\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages and modules\n",
    "import torch\n",
    "from titan.memory_modules import MAGModule, MACModule, MALModule\n",
    "\n",
    "# Set device (CPU/GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model hyperparameters\n",
    "d_model = 64  # Hidden size\n",
    "batch_size = 2\n",
    "seq_len = 32\n",
    "\n",
    "# Instantiate the model variant, e.g., MAGModule\n",
    "model = MAGModule(d_model=d_model).to(device)\n",
    "\n",
    "# Print total number of trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")\n",
    "\n",
    "# Create dummy input data\n",
    "x = torch.randn(batch_size, seq_len, d_model).to(device)\n",
    "\n",
    "# Run a forward pass\n",
    "output, mem_loss = model(x)\n",
    "\n",
    "# Print output shape and associative memory loss\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Associative memory loss:\", mem_loss.item())\n",
    "\n",
    "# --- Optionally, test other variants ---\n",
    "\n",
    "# Testing MACModule:\n",
    "# For MAC, simulate current segment and historical memory.\n",
    "current_segment = x[:, :16, :]\n",
    "historical_memory = x[:, 16:, :]  # Dummy historical memory for demonstration\n",
    "mac_model = MACModule(d_model=d_model).to(device)\n",
    "mac_output = mac_model(current_segment, historical_memory)\n",
    "print(\"MAC output shape:\", mac_output.shape)\n",
    "\n",
    "# Testing MALModule:\n",
    "mal_model = MALModule(d_model=d_model).to(device)\n",
    "mal_output, mal_mem_loss = mal_model(x)\n",
    "print(\"MAL output shape:\", mal_output.shape)\n",
    "print(\"MAL associative memory loss:\", mal_mem_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train Loss: 0.4591  Train Accuracy: 0.7748\n",
      "  Val Loss:   0.4756  Val Accuracy:   0.7718\n",
      "Epoch 2:\n",
      "  Train Loss: 0.2717  Train Accuracy: 0.8901\n",
      "  Val Loss:   0.4942  Val Accuracy:   0.7993\n",
      "Epoch 3:\n",
      "  Train Loss: 0.2056  Train Accuracy: 0.9201\n",
      "  Val Loss:   0.6838  Val Accuracy:   0.7901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rahul Patnaik\\Titan Architehcture\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Rahul Patnaik\\.cache\\huggingface\\hub\\datasets--glue. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "\n",
      "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]\n",
      "Generating train split: 100%|██████████| 67349/67349 [00:00<00:00, 1413513.44 examples/s]\n",
      "\n",
      "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]\n",
      "Generating validation split: 100%|██████████| 872/872 [00:00<00:00, 176909.79 examples/s]\n",
      "\n",
      "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]\n",
      "Generating test split: 100%|██████████| 1821/1821 [00:00<00:00, 672403.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "!python trainMAG.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rahul Patnaik\\Titan Architehcture\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7901\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from titan.memory_modules import MAGModule  # or change to MACModule or MALModule as needed\n",
    "\n",
    "# Define the Titan-based classifier model (same as used during training)\n",
    "class TitanClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_classes, max_length):\n",
    "        super(TitanClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # Here, we use the MAG variant; change as needed\n",
    "        self.titan = MAGModule(d_model=d_model)\n",
    "        # Classification head: average pooled representation -> linear layer\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (batch, seq_len)\n",
    "        x = self.embedding(input_ids)  # (batch, seq_len, d_model)\n",
    "        titan_out, mem_loss = self.titan(x)  # (batch, seq_len, d_model), scalar memory loss (aggregated)\n",
    "        # Mean pooling over the sequence dimension\n",
    "        x_pooled = titan_out.mean(dim=1)  # (batch, d_model)\n",
    "        logits = self.classifier(x_pooled)  # (batch, num_classes)\n",
    "        return logits, mem_loss\n",
    "\n",
    "# Define a Dataset class for SST2 (validation split)\n",
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, split, tokenizer, max_length):\n",
    "        self.samples = load_dataset(\"glue\", \"sst2\")[split]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.samples[idx][\"sentence\"]\n",
    "        label = self.samples[idx][\"label\"]  # For validation, labels are provided.\n",
    "        encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)  # shape: (max_length)\n",
    "        return input_ids, label\n",
    "\n",
    "# Hyperparameters (should match those used during training)\n",
    "d_model = 64\n",
    "num_classes = 2\n",
    "max_length = 64\n",
    "batch_size = 16\n",
    "\n",
    "# Load tokenizer and get vocab size\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Instantiate the model and load saved weights\n",
    "model = TitanClassifier(vocab_size, d_model, num_classes, max_length)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\"titan_classifier.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Create the SST2 validation dataset and loader\n",
    "val_dataset = SST2Dataset(\"validation\", tokenizer, max_length)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "all_logits = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for input_ids, labels in val_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits, _ = model(input_ids)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += input_ids.size(0)\n",
    "        all_logits.append(logits.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "accuracy = total_correct / total_samples\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Optionally, aggregate logits and labels for further analysis\n",
    "all_logits = np.concatenate(all_logits, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# You could also compute other metrics (e.g., F1 score) if desired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train Loss: 0.4561  Train Accuracy: 0.7825\n",
      "  Val Loss:   0.4548  Val Accuracy:   0.7924\n",
      "Epoch 2:\n",
      "  Train Loss: 0.2780  Train Accuracy: 0.8876\n",
      "  Val Loss:   0.4573  Val Accuracy:   0.7867\n",
      "Epoch 3:\n",
      "  Train Loss: 0.2144  Train Accuracy: 0.9166\n",
      "  Val Loss:   0.4962  Val Accuracy:   0.7878\n"
     ]
    }
   ],
   "source": [
    "!python trainMAC.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7878\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Import the MAC variant classifier.\n",
    "# If you defined TitanClassifierMAC in your train_mac.py, ensure it's accessible in your PYTHONPATH.\n",
    "from titan.memory_modules import MACModule\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define TitanClassifierMAC (same as in train_mac.py)\n",
    "class TitanClassifierMAC(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_classes, max_length, persistent_len=10):\n",
    "        super(TitanClassifierMAC, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # MAC module from your titan package\n",
    "        self.mac = MACModule(d_model, persistent_len=persistent_len)\n",
    "        # Classification head: average pooling then a linear layer\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (batch, seq_len)\n",
    "        x = self.embedding(input_ids)  # (batch, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        half = seq_len // 2\n",
    "        current_segment = x[:, :half, :]       # First half: current segment\n",
    "        historical_memory = x[:, half:, :]       # Second half: historical memory\n",
    "        \n",
    "        # Pass segments through the MAC module.\n",
    "        # MACModule concatenates persistent memory, historical memory, and current segment.\n",
    "        mac_out = self.mac(current_segment, historical_memory)  # (batch, persistent_len + seq_len, d_model)\n",
    "        \n",
    "        # Mean pooling over the token dimension.\n",
    "        pooled = mac_out.mean(dim=1)  # (batch, d_model)\n",
    "        logits = self.classifier(pooled)  # (batch, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Custom dataset for SST2\n",
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, split, tokenizer, max_length):\n",
    "        self.samples = load_dataset(\"glue\", \"sst2\")[split]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.samples[idx][\"sentence\"]\n",
    "        label = self.samples[idx][\"label\"]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        return input_ids, label\n",
    "\n",
    "# Hyperparameters (must match training configuration)\n",
    "d_model = 64\n",
    "num_classes = 2\n",
    "max_length = 64\n",
    "batch_size = 16\n",
    "\n",
    "# Load tokenizer and dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "val_dataset = SST2Dataset(\"validation\", tokenizer, max_length)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Instantiate the model and load saved weights.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TitanClassifierMAC(vocab_size, d_model, num_classes, max_length).to(device)\n",
    "model.load_state_dict(torch.load(\"titan_classifier_mac.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model on the validation set.\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, labels in val_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = model(input_ids)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += input_ids.size(0)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "accuracy = total_correct / total_samples\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Optionally, you can further compute metrics like F1-score, confusion matrix, etc.\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train Loss: 0.4594  Train Accuracy: 0.7784\n",
      "  Val Loss:   0.4682  Val Accuracy:   0.7867\n",
      "Epoch 2:\n",
      "  Train Loss: 0.2732  Train Accuracy: 0.8880\n",
      "  Val Loss:   0.5034  Val Accuracy:   0.7867\n",
      "Epoch 3:\n",
      "  Train Loss: 0.2084  Train Accuracy: 0.9184\n",
      "  Val Loss:   0.5879  Val Accuracy:   0.7752\n"
     ]
    }
   ],
   "source": [
    "!python trainMAL.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7752\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Define TitanClassifierMAL (should match your training definition)\n",
    "# This model uses the MAL variant from your titan.memory_modules module.\n",
    "from titan.memory_modules import MALModule\n",
    "\n",
    "class TitanClassifierMAL(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_classes, max_length):\n",
    "        super(TitanClassifierMAL, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # MALModule acts as a standalone memory layer.\n",
    "        self.mal = MALModule(d_model=d_model)\n",
    "        # Classification head: we apply mean pooling over the sequence and then a linear layer.\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # input_ids shape: (batch, seq_len)\n",
    "        x = self.embedding(input_ids)  # shape: (batch, seq_len, d_model)\n",
    "        mal_out, mem_loss = self.mal(x)  # mal_out: (batch, seq_len, d_model)\n",
    "        pooled = mal_out.mean(dim=1)     # shape: (batch, d_model)\n",
    "        logits = self.classifier(pooled) # shape: (batch, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Custom dataset for SST2 validation.\n",
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, split, tokenizer, max_length):\n",
    "        self.samples = load_dataset(\"glue\", \"sst2\")[split]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.samples[idx][\"sentence\"]\n",
    "        label = self.samples[idx][\"label\"]\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  truncation=True,\n",
    "                                  padding=\"max_length\",\n",
    "                                  max_length=self.max_length,\n",
    "                                  return_tensors=\"pt\")\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)  # shape: (max_length)\n",
    "        return input_ids, label\n",
    "\n",
    "# Hyperparameters (must match those used during training)\n",
    "d_model = 64\n",
    "num_classes = 2\n",
    "max_length = 64\n",
    "batch_size = 16\n",
    "\n",
    "# Load tokenizer and prepare dataset and DataLoader.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "val_dataset = SST2Dataset(\"validation\", tokenizer, max_length)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Instantiate the model and load saved weights.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TitanClassifierMAL(vocab_size, d_model, num_classes, max_length).to(device)\n",
    "model.load_state_dict(torch.load(\"titan_classifier_mal.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on the validation set.\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, labels in val_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = model(input_ids)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += input_ids.size(0)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "accuracy = total_correct / total_samples\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Optionally, you can aggregate predictions and labels for further metrics.\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
